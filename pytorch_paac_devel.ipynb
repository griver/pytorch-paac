{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import numpy as np\n",
    "import copy \n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from environment_creator import EnvironmentCreator\n",
    "import argparse\n",
    "\n",
    "def bool_arg(string):\n",
    "    value = string.lower()\n",
    "    if value == 'true':\n",
    "        return True\n",
    "    elif value == 'false':\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError(\"Expected True or False, but got {}\".format(string))\n",
    "\n",
    "def get_arg_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-g', default='pong', help='Name of game', dest='game')\n",
    "    parser.add_argument('-d', '--device', default='/gpu:0', type=str, help=\"Device to be used ('/cpu:0', '/gpu:0', '/gpu:1',...)\", dest=\"device\")\n",
    "    parser.add_argument('--rom_path', default='./atari_roms', help='Directory where the game roms are located (needed for ALE environment)', dest=\"rom_path\")\n",
    "    parser.add_argument('-v', '--visualize', default=False, type=bool_arg, help=\"0: no visualization of emulator; 1: all emulators, for all actors, are visualized; 2: only 1 emulator (for one of the actors) is visualized\", dest=\"visualize\")\n",
    "    parser.add_argument('--e', default=0.1, type=float, help=\"Epsilon for the Rmsprop and Adam optimizers\", dest=\"e\")\n",
    "    parser.add_argument('--alpha', default=0.99, type=float, help=\"Discount factor for the history/coming gradient, for the Rmsprop optimizer\", dest=\"alpha\")\n",
    "    parser.add_argument('-lr', '--initial_lr', default=0.0224, type=float, help=\"Initial value for the learning rate. Default = 0.0224\", dest=\"initial_lr\")\n",
    "    parser.add_argument('-lra', '--lr_annealing_steps', default=80000000, type=int, help=\"Nr. of global steps during which the learning rate will be linearly annealed towards zero\", dest=\"lr_annealing_steps\")\n",
    "    parser.add_argument('--entropy', default=0.02, type=float, help=\"Strength of the entropy regularization term (needed for actor-critic)\", dest=\"entropy_regularisation_strength\")\n",
    "    parser.add_argument('--clip_norm', default=3.0, type=float, help=\"If clip_norm_type is local/global, grads will be clipped at the specified maximum (avaerage) L2-norm\", dest=\"clip_norm\")\n",
    "    parser.add_argument('--clip_norm_type', default=\"global\", help=\"Whether to clip grads by their norm or not. Values: ignore (no clipping), local (layer-wise norm), global (global norm)\", dest=\"clip_norm_type\")\n",
    "    parser.add_argument('--gamma', default=0.99, type=float, help=\"Discount factor\", dest=\"gamma\")\n",
    "    parser.add_argument('--max_global_steps', default=80000000, type=int, help=\"Max. number of training steps\", dest=\"max_global_steps\")\n",
    "    parser.add_argument('--max_local_steps', default=5, type=int, help=\"Number of steps to gain experience from before every update.\", dest=\"max_local_steps\")\n",
    "    parser.add_argument('--arch', default='NIPS', help=\"Which network architecture to use: from the NIPS or NATURE paper\", dest=\"arch\")\n",
    "    parser.add_argument('--single_life_episodes', default=False, type=bool_arg, help=\"If True, training episodes will be terminated when a life is lost (for games)\", dest=\"single_life_episodes\")\n",
    "    parser.add_argument('-ec', '--emulator_counts', default=32, type=int, help=\"The amount of emulators per agent. Default is 32.\", dest=\"emulator_counts\")\n",
    "    parser.add_argument('-ew', '--emulator_workers', default=8, type=int, help=\"The amount of emulator workers per agent. Default is 8.\", dest=\"emulator_workers\")\n",
    "    parser.add_argument('-df', '--debugging_folder', default='logs/', type=str, help=\"Folder where to save the debugging information.\", dest=\"debugging_folder\")\n",
    "    parser.add_argument('-rs', '--random_start', default=True, type=bool_arg, help=\"Whether or not to start with 30 noops for each env. Default True\", dest=\"random_start\")\n",
    "    return parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creator arguments:\n",
      "rom_path: ./atari_roms\n",
      "game: pong\n",
      "random_seed: 3\n",
      "random_start: True\n",
      "single_life_episodes: False\n",
      "visualize: False\n"
     ]
    }
   ],
   "source": [
    "def public_methods(obj):\n",
    "    for m in dir(obj):\n",
    "        if not m.startswith('_'):\n",
    "            print(m)\n",
    "\n",
    "def init_args(random_seed=3):\n",
    "    args = get_arg_parser().parse_args(['-g', 'pong', '-df', 'logs/'])\n",
    "    args.random_seed = random_seed\n",
    "    return args\n",
    "\n",
    "def print_env_associated_args(args):\n",
    "    for arg in ['rom_path', 'game', 'random_seed', 'random_start', 'single_life_episodes', 'visualize']:\n",
    "        value = getattr(args, arg)\n",
    "        print('%s:' % arg, value)\n",
    "        \n",
    "def get_EnvironmentCreator():\n",
    "    args = init_args()\n",
    "    print('Creator arguments:')\n",
    "    print_env_associated_args(args)\n",
    "    env_creator = EnvironmentCreator(args)\n",
    "    return env_creator\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "env_creator = get_EnvironmentCreator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "env = env_creator.create_environment(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 4) uint8\n",
      "236 87\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "state = env.get_initial_state()\n",
    "print(state.shape, state.dtype)\n",
    "print(state.max(), state.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6IAAADxCAYAAADRCgpDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJElSUzaikiRJkqSm/h/hVc6MyTLD/gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f88da59f550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step39\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_state(state, width=16, height=8):\n",
    "    _,_, T = state.shape\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    for i in range(T):\n",
    "        a = fig.add_subplot(1,T,i+1)\n",
    "        plt.imshow(state[:,:,i], cmap='gray')\n",
    "        a.set_title('frame#%d '% (i+1))\n",
    "    plt.show()\n",
    "\n",
    "env.get_initial_state()\n",
    "for i in range(40):\n",
    "    obs, r, terminal = env.next(env.get_noop())\n",
    "    show_state(obs)\n",
    "    print('step%d' % i)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.init as nn_init\n",
    "\n",
    "def init_conv2d(module):\n",
    "    (h, w), c = module.kernel_size, module.in_channels\n",
    "    d = 1.0 / np.sqrt(c*h*w)\n",
    "    \n",
    "    nn_init.uniform(module.weight, -d, d)\n",
    "    nn_init.uniform(module.bias, -d, d)\n",
    "    \n",
    "\n",
    "def init_linear(module):\n",
    "    d = 1.0 / np.sqrt(module.in_features)\n",
    "    nn_init.uniform(module.weight, -d, d)\n",
    "    nn_init.uniform(module.bias, -d, d)\n",
    "\n",
    "    \n",
    "def init_model_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        init_linear(module)\n",
    "    elif isinstance(module, nn.Conv2d):\n",
    "        init_conv2d(module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_state(s):\n",
    "    # We need s.transpose because AtariEmulator returns states with shape (H,W,C), but\n",
    "    # pytorch conv layers expect inputs of shape (C,H,W)\n",
    "    # if needed it is possible to change AtariEmulator output shape via changing the ObservationPool class.\n",
    "    return torch.from_numpy(np.ascontiguousarray(s.transpose(2,0,1),dtype=np.float32)/255.)\n",
    "\n",
    "class FFNetwork(nn.Module):\n",
    "    def __init__(self, config, input_channels=4):\n",
    "        super(FFNetwork, self).__init__()\n",
    "        self._conf = config\n",
    "        self._loss_scaling = 5.0\n",
    "        self._create_network(input_channels)\n",
    "        self.apply(init_model_weights)\n",
    "    \n",
    "    def _create_network(self, in_channels):\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, (8,8), stride=4)\n",
    "        self.conv2 = nn.Conv2d(16, 32, (4,4), stride=2)\n",
    "        #if input shape equals (4, 84,84) then the conv part output shape is: (32, 9, 9)\n",
    "        #self.__flatten_conv_size = 32*9*9 \n",
    "        self.fc3 = nn.Linear(32*9*9, 256)\n",
    "        self.fc_policy = nn.Linear(256, self._conf['num_actions'])\n",
    "        self.fc_value = nn.Linear(256, 1)\n",
    "      \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.conv1(inputs))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        prob = F.softmax(self.fc_policy(x)) #in pytorch A3C an author just outputs logits(the softmax input).\n",
    "        value = self.fc_value(x)\n",
    "        return value, prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== network_conf ====================\n",
      "num_actions: 6\n",
      "device: 0\n",
      "clip_norm: 3.0\n",
      "entropy_regularisation_strength: 0.02\n",
      "clip_norm_type: global\n",
      "name: PAAC\n",
      "======================================================\n",
      "args.device: 0\n",
      "Value: Variable containing:\n",
      "1.00000e-02 *\n",
      " -2.1845\n",
      "[torch.cuda.FloatTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "prob: Variable containing:\n",
      " 0.1692  0.1659  0.1646  0.1583  0.1678  0.1743\n",
      "[torch.cuda.FloatTensor of size 1x6 (GPU 0)]\n",
      "\n",
      "Model:\n",
      "FFNetwork (\n",
      "  (conv1): Conv2d(4, 16, kernel_size=(8, 8), stride=(4, 4))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2))\n",
      "  (fc3): Linear (2592 -> 256)\n",
      "  (fc_policy): Linear (256 -> 6)\n",
      "  (fc_value): Linear (256 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def print_config(config_name, dictionary):\n",
    "    \n",
    "    def print_dict(d, predicate=''):\n",
    "        for k, v in d.items():\n",
    "            print(predicate + '%s:' % k, end=' ')\n",
    "            if isinstance(v, dict):\n",
    "                print()\n",
    "                print_dict(v, predicate + '  ')\n",
    "            else:\n",
    "                print(str(v))\n",
    "    \n",
    "    header = '{0} {1} {0}'.format('=='*10, config_name)\n",
    "    print(header)\n",
    "    print_dict(dictionary)\n",
    "    print('='*len(header))\n",
    "    \n",
    "    \n",
    "def test_FFNetwork(env, num_actions, random_seed=3):\n",
    "    import network\n",
    "    \n",
    "    args = init_args()\n",
    "    args.num_actions = num_actions\n",
    "    args.random_seed = random_seed\n",
    "    network_conf = {'num_actions': args.num_actions,\n",
    "                    'entropy_regularisation_strength': args.entropy_regularisation_strength,\n",
    "                    'device': 0,\n",
    "                    'clip_norm': args.clip_norm,\n",
    "                    'clip_norm_type': args.clip_norm_type,\n",
    "                    'name':'PAAC'}\n",
    "    gpu_id = network_conf['device']\n",
    "    Tensor = torch.FloatTensor if gpu_id < 0 else torch.cuda.FloatTensor\n",
    "    \n",
    "    print_config('network_conf', network_conf)\n",
    "    my_model = network.FFNetwork(copy.copy(network_conf))\n",
    "    print('args.device:', network_conf['device'])\n",
    "    my_model.cuda(network_conf['device'])\n",
    "    state = env.get_initial_state()\n",
    "    state = process_state(state).unsqueeze(0)\n",
    "    value, act_logits = my_model(Variable(state).type(Tensor))\n",
    "    \n",
    "    print('Value:', value)\n",
    "    print('prob:', F.softmax(act_logits))\n",
    "    print('Model:')\n",
    "    print(my_model)\n",
    "    \n",
    "    return my_model\n",
    "    \n",
    "    \n",
    "model = test_FFNetwork(env, env_creator.num_actions)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compare_ways_of_creating_EmulatorRunners_inputs(res1, res2):\n",
    "    if len(res1) != len(res2):\n",
    "        return False\n",
    "    \n",
    "    cmp_funs = [\n",
    "        lambda i1, i2: i1 == i2, #indices are equal\n",
    "        lambda ems1, ems2: np.array_equal(ems1, ems2), #emulators are equal\n",
    "        lambda vars1, vars2: all(np.array_equal(v1,v2) for v1, v2 in zip(vars1, vars2))\n",
    "    ]\n",
    "    \n",
    "    for i in range(len(res1)):\n",
    "        worker_i_res_are_equal = all(fun(res1[i][j], res2[i][j]) for j, fun in enumerate(cmp_funs))\n",
    "        if worker_i_res_are_equal is False:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "#%pdoc np.finfo\n",
    "del env, env_creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available: True\n",
      "num_gpu: 1\n",
      "cuda tensor:\n",
      "\n",
      "-0.2147 -0.2638 -1.1271\n",
      "-1.1010 -0.9133  1.0634\n",
      "-0.1374 -0.9373 -0.5536\n",
      "[torch.cuda.FloatTensor of size 3x3 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def remove_checkpoint():\n",
    "    from paac import PAACLearner\n",
    "    checkpoint = 'logs/checkpoints/'+PAACLearner.CHECKPOINT_LAST\n",
    "    if os.path.isfile(checkpoint):\n",
    "        os.remove(checkpoint)\n",
    "\n",
    "print('cuda is available:', torch.cuda.is_available())\n",
    "print('num_gpu:', torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda tensor:', torch.randn(3,3).cuda(), sep='\\n')\n",
    "    \n",
    "remove_checkpoint()\n",
    "\n",
    "shutil.rmtree('logs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n",
      "DEBUG:root:get_network_and_environment_creator\n",
      "DEBUG:root:PAAC init is started\n",
      "DEBUG:root:Moved model computations on a GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGS:\n",
      "\"emulator_counts\": 16\n",
      "\"device\": gpu\n",
      "\"random_seed\": 3\n",
      "\"max_local_steps\": 5\n",
      "\"clip_norm\": 3.0\n",
      "\"entropy_regularisation_strength\": 0.02\n",
      "\"clip_norm_type\": global\n",
      "\"initial_lr\": 0.0224\n",
      "\"rom_path\": ./atari_roms\n",
      "\"gamma\": 0.99\n",
      "\"single_life_episodes\": False\n",
      "\"lr_annealing_steps\": 80000000\n",
      "\"random_start\": True\n",
      "\"visualize\": False\n",
      "\"e\": 0.1\n",
      "\"max_global_steps\": 105000\n",
      "\"emulator_workers\": 4\n",
      "\"game\": pong\n",
      "\"alpha\": 0.99\n",
      "\"debugging_folder\": logs/\n",
      "\n",
      "==================== network_conf ====================\n",
      "num_actions: 6\n",
      "device: gpu\n",
      "clip_norm: 3.0\n",
      "entropy_regularisation_strength: 0.02\n",
      "clip_norm_type: global\n",
      "name: paac_network\n",
      "======================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Paac init is done!\n",
      "DEBUG:root:Starting training at step 0\n",
      "DEBUG:root:Creating runners...\n",
      "INFO:root:Ran 10240 steps, at 1163.1458679977816 steps/s (1174.8949961401875 steps/s avg), last 10 rewards avg 0.0\n",
      "INFO:root:Ran 20480 steps, at 1232.4860238751148 steps/s (1173.3467932761662 steps/s avg), last 10 rewards avg -20.3\n",
      "INFO:root:Ran 30720 steps, at 1254.3478439655335 steps/s (1176.0405866607907 steps/s avg), last 10 rewards avg -20.0\n",
      "INFO:root:Ran 40960 steps, at 681.5407489255218 steps/s (1155.8986887050364 steps/s avg), last 10 rewards avg -20.1\n",
      "INFO:root:Ran 51200 steps, at 1201.5050667812511 steps/s (1132.97968973752 steps/s avg), last 10 rewards avg -20.1\n",
      "INFO:root:Ran 61440 steps, at 842.9893402940903 steps/s (1124.9876701591186 steps/s avg), last 10 rewards avg -20.4\n",
      "INFO:root:Ran 71680 steps, at 1165.1653587054657 steps/s (1110.5334029881144 steps/s avg), last 10 rewards avg -20.3\n",
      "INFO:root:Ran 81920 steps, at 1050.2531229557198 steps/s (1108.0193227916209 steps/s avg), last 10 rewards avg -20.1\n",
      "INFO:root:Ran 92160 steps, at 915.0726234434911 steps/s (1105.5039125827373 steps/s avg), last 10 rewards avg -20.1\n",
      "INFO:root:Ran 102400 steps, at 1046.5613693639743 steps/s (1101.6634511186999 steps/s avg), last 10 rewards avg -20.4\n",
      "INFO:root:The state of the agent is saved at step #105040\n",
      "DEBUG:root:Training ended at step 105040\n",
      "INFO:root:The state of the agent is saved at step #105040\n",
      "INFO:root:Cleanup completed...\n",
      "DEBUG:root:Deleting PAACLearner, env_creator, args...\n",
      "DEBUG:root:DONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runners:\n",
      "[<EmulatorRunner(EmulatorRunner-13, started)>, <EmulatorRunner(EmulatorRunner-14, started)>, <EmulatorRunner(EmulatorRunner-15, started)>, <EmulatorRunner(EmulatorRunner-16, started)>]\n"
     ]
    }
   ],
   "source": [
    "import time, copy \n",
    "import network as network\n",
    "\n",
    "def print_args(args):\n",
    "    print('ARGS:')\n",
    "    for key,val in vars(args).items():\n",
    "        print('\"{0}\": {1}'.format(key,val))\n",
    "    print()\n",
    "    \n",
    "def parse_args(command_line='-g pong -df logs/', random_seed=3):\n",
    "    args = get_arg_parser().parse_args(command_line.split())\n",
    "    args.random_seed = random_seed\n",
    "    return args\n",
    "\n",
    "def adapt_args(args):\n",
    "    '''Адаптируем аргументы для тестирования моего кода'''\n",
    "    args.device = 'gpu' #Interprets the number as GPU identifier, if negative value is given, uses cpu. .\n",
    "    del args.arch\n",
    "    \n",
    "\n",
    "def get_network_and_environment_creator(args):\n",
    "    logging.debug('get_network_and_environment_creator')\n",
    "    env_creator = EnvironmentCreator(args)\n",
    "    args.num_actions = env_creator.num_actions\n",
    "\n",
    "    network_conf = {'num_actions': args.num_actions,\n",
    "                    'entropy_regularisation_strength': args.entropy_regularisation_strength,\n",
    "                    'device': args.device,\n",
    "                    'clip_norm': args.clip_norm,\n",
    "                    'clip_norm_type': args.clip_norm_type,\n",
    "                    'name': 'paac_network'}\n",
    "    \n",
    "    print_config('network_conf', network_conf)\n",
    "    \n",
    "    def network_creator():\n",
    "        net = network.FFNetwork(copy.copy(network_conf))\n",
    "        if network_conf['device'] == 'gpu':\n",
    "            net = net.cuda()\n",
    "            logging.debug('Moved model computations on a GPU')\n",
    "        return net\n",
    "    \n",
    "    return network_creator, env_creator\n",
    "\n",
    "def cleanup(learner, env_creator, args):\n",
    "    learner.cleanup()\n",
    "    logging.info('Cleanup completed...')\n",
    "    logging.debug('Deleting PAACLearner, env_creator, args...')\n",
    "    del learner, env_creator, args\n",
    "\n",
    "def process_states(s):\n",
    "    # We need s.transpose because shared_states are given in shape (batch, H,W,C), but\n",
    "    # pytorch conv layers expect inputs of shape (batch, C,H,W)\n",
    "    # if needed it is possible to change AtariEmulator output shape via changing the ObservationPool class.\n",
    "    return torch.from_numpy(np.ascontiguousarray(s.transpose(0,3,1,2),dtype=np.float32)/255.)\n",
    "\n",
    "\n",
    "def choose_action(self, states):\n",
    "    num_actions = self.args['num_actions']\n",
    "    V_state, action_probs = self.network(Variable(process_states(states)))\n",
    "    acts_one_hot = np.array([np.random.multinomial(1,p.data.numpy()) for p in action_probs])\n",
    "    return acts_one_hot, V_state, action_probs\n",
    "\n",
    "def test_train_function(self):\n",
    "    from emulator_runner import EmulatorRunner\n",
    "    from runners import Runners\n",
    "    \n",
    "    start_step = step = self.global_step + 1\n",
    "    logging.debug('Starting training at step %d' % start_step)\n",
    "    num_emulators = self.args['emulator_counts']\n",
    "    num_actions = self.args['num_actions']\n",
    "    max_local_steps = self.args['max_local_steps']\n",
    "    max_global_steps = self.args['max_global_steps']\n",
    "    total_rewards = []\n",
    "        \n",
    "    #states, rewards, is_terminal, actions\n",
    "    variables = [\n",
    "        np.asarray([em.get_initial_state() for em in self.emulators], dtype=np.uint8),\n",
    "        np.zeros(self.args['emulator_counts'], dtype=np.float32),\n",
    "        np.asarray([False] * self.args['emulator_counts'], dtype=np.float32),\n",
    "        np.zeros((self.args['emulator_counts'], self.args['num_actions']), dtype=np.float32)\n",
    "    ]\n",
    "    logging.debug('Creating runners...')\n",
    "    self.runners = Runners(EmulatorRunner, self.emulators, self.num_workers, variables)\n",
    "    self.runners.start()\n",
    "\n",
    "    shared_vars = self.runners.get_shared_variables()\n",
    "    shared_s, shared_r, shared_done, shared_a = shared_vars\n",
    "    #any summaries here?\n",
    "    emulator_steps = [0] * num_emulators\n",
    "    total_episode_rewards = [0] * num_emulators\n",
    "\n",
    "    actions_sum = np.zeros((num_emulators, num_actions))\n",
    "    y_batch = np.zeros((max_local_steps, num_emulators))\n",
    "    adv_batch = np.zeros((max_local_steps, num_emulators))\n",
    "    rewards = np.zeros((max_local_steps, num_emulators))\n",
    "    states = np.zeros([max_local_steps] + list(shared_s.shape), dtype=np.uint8)\n",
    "    actions = np.zeros((max_local_steps, num_emulators, num_actions))\n",
    "    values = np.zeros((max_local_steps, num_emulators))\n",
    "    not_done_masks = np.zeros((max_local_steps, num_emulators))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    #while self.global_step < max_global_steps:\n",
    "\n",
    "    #    loop_time = time.time()\n",
    "\n",
    "    for t in range(max_local_steps):\n",
    "        #print(t, 'shared_states shape:', shared_s.shape)\n",
    "        print('========='*5)\n",
    "        a_t, v_t, probs_t, log_probs_t = self.choose_action(shared_s)                       \n",
    "        actions_sum += a_t\n",
    "        shared_a[:] = a_t[:]\n",
    "        actions[t] = a_t\n",
    "        values[t] = v_t.view(-1).data.numpy()\n",
    "        states[t] = shared_s\n",
    "        print('actions_one_hot:', a_t, sep='\\n')\n",
    "        print('actions_id:', np.argmax(a_t, 1))\n",
    "        self.runners.update_environments()\n",
    "        self.runners.wait_updated()\n",
    "        \n",
    "        \n",
    "        not_done_masks[t] = 1.0 - shared_done.astype(np.float32)\n",
    "        \n",
    "        for e, (actual_reward, episode_over) in enumerate(zip(shared_r, shared_done)):\n",
    "            total_episode_rewards[e] += actual_reward\n",
    "            actual_reward = np.clip(actual_reward, -1.,1.)\n",
    "            rewards[t, e] = actual_reward\n",
    "\n",
    "            emulator_steps[e] += 1\n",
    "            self.global_step += 1\n",
    "            if episode_over:\n",
    "                total_rewards.append(total_episode_rewards[e])\n",
    "                total_episode_rewards[e] = 0\n",
    "                emulator_steps[e] = 0\n",
    "                actions_sum[e] = np.zeros(num_actions)\n",
    "    \n",
    "    nest_state_value, _ = self.network(Variable(process_states(shared_s)))\n",
    "    #nest_state_value = nest_state_value.detach()\n",
    "    estimated_return = np.copy(nest_state_value.data.numpy()).squeeze()\n",
    "    \n",
    "    #fill the targets arrays:\n",
    "    for t in reversed(range(max_local_steps)):\n",
    "        estimated_return = rewards[t] + self.gamma * estimated_return * not_done_masks[t]\n",
    "        y_batch[t] = estimated_return\n",
    "        adv_batch[t] = estimated_return - values[t]\n",
    "        \n",
    "    flat_states = states.reshape((-1,) + states.shape[2:])\n",
    "    flat_y_batch = y_batch.reshape(-1)\n",
    "    flat_adv_batch = adv_batch.reshape(-1)\n",
    "    flat_actions = actions.reshape((-1,) + actions.shape[2:])\n",
    "    #print('flat_actions:', flat_actions, sep='\\n')\n",
    "    \n",
    "    #lr = self.get_lr()\n",
    "    #feed_dict = {self.network.input_ph: flat_states,\n",
    "    #             self.network.critic_target_ph: flat_y_batch,\n",
    "    #             self.network.selected_action_ph: flat_actions,\n",
    "    #             self.network.adv_actor_ph: flat_adv_batch,\n",
    "    #             self.learning_rate: lr}\n",
    "    #\n",
    "    #_, summaries = self.session.run([self.train_step, summaries_op], feed_dict=feed_dict)\n",
    "           \n",
    "    logging.debug('Training ended at step %d' % step)\n",
    "    \n",
    "\n",
    "def test_paac_and_gradient_clipping(command_line):\n",
    "    from utils import clip_local_grad_norm\n",
    "    from paac import PAACLearner\n",
    "    # Configure logging module to output messages in Jupyter Notebook:\n",
    "    import logging \n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logging.debug(\"test\")\n",
    "    # -ec = ew = 1 because we need only one emulator for testing purposes\n",
    "    args = parse_args(command_line) \n",
    "    adapt_args(args)\n",
    "    print_args(args)\n",
    "    try:\n",
    "        net_creator, env_creator = get_network_and_environment_creator(args)\n",
    "        learner = PAACLearner(net_creator, env_creator, args)\n",
    "        \n",
    "        #test_train_function(learner)\n",
    "        learner.train()\n",
    "        print('Runners:')\n",
    "        print(learner.runners.runners)\n",
    "        \n",
    "        #net = net_creator()\n",
    "        #if args.device >= 0:\n",
    "        #    print('use cuda({0})'.format(args.device))\n",
    "        #    net.cuda(args.device)\n",
    "        #print('Network:', net, sep='\\n')\n",
    "    finally:\n",
    "        cleanup(learner, env_creator, args)\n",
    "        pass \n",
    "    \n",
    "    logging.debug(\"DONE\")\n",
    "\n",
    "command_line = '-g pong -df logs/ -ec 16 -ew 4'\n",
    "command_line += ' --max_global_steps 105000 -lra 80000000'\n",
    "#command_line += ' --clip_norm 0.005 --clip_norm_type ignore'\n",
    "remove_checkpoint()\n",
    "test_paac_and_gradient_clipping(command_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs shape: (10, 4)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'torch.LongTensor' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-3a1d0a54366a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#print( list(zip(unique, counts)) )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtestprobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-112-3a1d0a54366a>\u001b[0m in \u001b[0;36mtestprobs\u001b[0;34m(N)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#print(probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0macts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'acts:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.LongTensor' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "def testprobs(N = 10):\n",
    "    #lambda l: for e in l\n",
    "    \n",
    "    probs_list = []\n",
    "    probs_list.append(np.array([[0.5, 0.5, 0.,0.]]*N))# - np.finfo(np.float32).epsneg\n",
    "    #action_indexes = [int(np.nonzero(np.random.multinomial(1, p))[0]) for p in probs]\n",
    "    probs_list.append(np.tile([0.,0.,0.1,.9], (N,1)))\n",
    "    \n",
    "    probs = np.concatenate(probs_list)\n",
    "    \n",
    "    \n",
    "    print('probs shape:', probs.shape)\n",
    "    #print(probs)\n",
    "    tprobs = torch.from_numpy(probs)\n",
    "    acts = tprobs.multinomial(1)\n",
    "    print('acts:', acts, sep='\\n')\n",
    "    first = 0 \n",
    "    for p in probs_list:\n",
    "        print('distribution:', p[0])\n",
    "        last = first + len(p)\n",
    "        acts_p = acts[first: last]\n",
    "        sample_probs = [(acts_p == act).type(torch.FloatTensor).mean() for act in range(4)] \n",
    "        print('sample probs:', sample_probs)\n",
    "        first = last   \n",
    "   #print(actions)\n",
    "    #unique, counts  = np.unique(action_indexes, return_counts=True)\n",
    "    #print( list(zip(unique, counts)) )\n",
    "    \n",
    "testprobs(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 84, 84)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe9fd3039b0>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe9fd31eeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "foo = env.get_initial_state()\n",
    "foo = foo.transpose(2,0,1)\n",
    "print(foo.shape)\n",
    "plt.imshow(foo[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10240\n",
      "20480\n",
      "30720\n",
      "40960\n",
      "51200\n",
      "61440\n",
      "71680\n",
      "81920\n",
      "92160\n"
     ]
    }
   ],
   "source": [
    "def test_counter(max_global_steps, num_emulators = 8, max_local_steps=5):\n",
    "    global_step = 0\n",
    "    counter = 0\n",
    "    while global_step <= max_global_steps:\n",
    "        global_step = counter*max_local_steps*num_emulators\n",
    "        if counter % (2048/ num_emulators) == 0:\n",
    "            print(global_step)\n",
    "        counter += 1\n",
    "\n",
    "test_counter(100000, num_emulators=32, max_local_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Test tensorflow code parts from PAAC\n",
    "=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[policy_v_network.NIPSPolicyVNetwork,\n",
       " policy_v_network.PolicyVNetwork,\n",
       " networks.NIPSNetwork,\n",
       " networks.Network,\n",
       " object]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from policy_v_network import NIPSPolicyVNetwork, PolicyVNetwork, NIPSNetwork, Network\n",
    "from environment_creator import EnvironmentCreator\n",
    "from train import get_arg_parser\n",
    "import tensorflow as tf\n",
    "%load_ext autoreload\n",
    "%autoreload 0\n",
    "\n",
    "NIPSPolicyVNetwork.mro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def public_methods(obj):\n",
    "    for m in dir(obj):\n",
    "        if not m.startswith('_'):\n",
    "            print(m)\n",
    "\n",
    "def init_args(random_seed=3):\n",
    "    args = get_arg_parser().parse_args(['-g', 'pong', '-df', 'logs/'])\n",
    "    args.random_seed = random_seed\n",
    "    return args\n",
    "\n",
    "        \n",
    "def print_config(config_name, dictionary):\n",
    "    \n",
    "    def print_dict(d, predicate=''):\n",
    "        for k, v in d.items():\n",
    "            print(predicate + '%s:' % k, end=' ')\n",
    "            if isinstance(v, dict):\n",
    "                print()\n",
    "                print_dict(v, predicate + '  ')\n",
    "            else:\n",
    "                print(str(v))\n",
    "    \n",
    "    header = '{0} {1} {0}'.format('=='*10, config_name)\n",
    "    print(header)\n",
    "    print_dict(dictionary)\n",
    "    print('='*len(header))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== network_conf ====================\n",
      "entropy_regularisation_strength: 0.02\n",
      "name: nips_network\n",
      "clip_norm: 3.0\n",
      "device: /gpu:0\n",
      "num_actions: 6\n",
      "clip_norm_type: global\n",
      "======================================================\n",
      "Network INIT\n",
      "NIPSNetwork INIT\n",
      "PolicyVNetwork INIT\n",
      "model name: nips_network\n"
     ]
    }
   ],
   "source": [
    "def prepare_network_conf(args, name, num_actions, random_seed=3):\n",
    "    args.num_actions = num_actions\n",
    "    args.random_seed = random_seed\n",
    "    network_conf = {'num_actions': args.num_actions,\n",
    "                    'entropy_regularisation_strength': args.entropy_regularisation_strength,\n",
    "                    'device': args.device,\n",
    "                    'clip_norm': args.clip_norm,\n",
    "                    'clip_norm_type': args.clip_norm_type,\n",
    "                    'name':name}\n",
    "    return network_conf\n",
    "\n",
    "def test_NIPSnetwork(num_actions, random_seed=3):\n",
    "    tf.reset_default_graph()\n",
    "    args = init_args()\n",
    "    network_conf = prepare_network_conf(args, 'nips_network', num_actions, random_seed)\n",
    "    print_config('network_conf', network_conf)\n",
    "    my_model = NIPSPolicyVNetwork(network_conf)\n",
    "    print('model name:', my_model.name)\n",
    "    \n",
    "test_NIPSnetwork(num_actions=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nips_network_1/conv1_weights:0 (8, 8, 4, 16)\n",
      "nips_network_1/conv1_biases:0 (16,)\n",
      "nips_network_1/conv2_weights:0 (4, 4, 16, 32)\n",
      "nips_network_1/conv2_biases:0 (32,)\n",
      "nips_network_1/fc3_weights:0 (2592, 256)\n",
      "nips_network_1/fc3_biases:0 (256,)\n",
      "nips_network_2/actor_output_weights:0 (256, 6)\n",
      "nips_network_2/actor_output_biases:0 (6,)\n",
      "nips_network_2/critic_output_weights:0 (256, 1)\n",
      "nips_network_2/critic_output_biases:0 (1,)\n"
     ]
    }
   ],
   "source": [
    "for v in tf.global_variables():\n",
    "    print(v.name, v.get_shape())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_RMSprop_params():\n",
    "    args = init_args()\n",
    "    optimizer_variable_names = 'OptimizerVariables'\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "    opt = tf.train.RMSPropOptimizer(learning_rate, decay=args.alpha, epsilon=args.e,\n",
    "                                                   name=optimizer_variable_names)\n",
    "    return opt\n",
    "opt = check_RMSprop_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE: \n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      "[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n",
      "\n",
      "AFTER: \n",
      " 1  0  0  0\n",
      " 0  1  0  0\n",
      " 0  0  1  0\n",
      " 0  0  0  1\n",
      "[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo = torch.zeros(4, 4).type(torch.cuda.FloatTensor)\n",
    "print('BEFORE:', foo)\n",
    "for t, row in enumerate(np.eye(4)):\n",
    "    foo[t] = torch.from_numpy(row).cuda()\n",
    "print('AFTER:', foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем выбор параметров через pytorch:\n",
    "=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parameters:\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3, 1])\n",
      "[[ 0.79346102]\n",
      " [ 0.76227766]\n",
      " [ 0.86646527]]\n",
      "output:\n",
      "probs:\n",
      "Variable containing:\n",
      " 0.3369  0.3457  0.3174\n",
      " 0.3271  0.3147  0.3581\n",
      " 0.3404  0.3588  0.3008\n",
      " 0.3371  0.3461  0.3168\n",
      "[torch.cuda.FloatTensor of size 4x3 (GPU 0)]\n",
      "\n",
      "actions: Variable containing:\n",
      " 0\n",
      " 1\n",
      " 2\n",
      " 2\n",
      "[torch.cuda.LongTensor of size 4x1 (GPU 0)]\n",
      "\n",
      "vectorized actions:\n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n",
      "shape: (4, 3)\n",
      "\n",
      "log_probs:\n",
      "Variable containing:\n",
      "-1.5227 -1.3345 -0.6566\n",
      "-0.7039 -0.9888 -2.0150\n",
      "-2.1359 -1.7469 -0.3459\n",
      "-1.5420 -1.3465 -0.6427\n",
      "[torch.cuda.FloatTensor of size 4x3 (GPU 0)]\n",
      "\n",
      "selected_log_probs: Variable containing:\n",
      "-1.5227\n",
      "-0.9888\n",
      "-0.3459\n",
      "-0.6427\n",
      "[torch.cuda.FloatTensor of size 4x1 (GPU 0)]\n",
      "\n",
      "rewards: Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.cuda.FloatTensor of size 4x1 (GPU 0)]\n",
      "\n",
      "actor_loss: Variable containing:\n",
      " 3.5001\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "GRADIENTS:\n",
      "MODEL SELECTING LOGS:\n",
      "   GRADS: None\n",
      "MODEL PROVIDING LOGS:\n",
      "   GRADS: Variable containing:\n",
      " 0.8715\n",
      "-1.5100\n",
      " 0.6385\n",
      "[torch.cuda.FloatTensor of size 3x1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_choose_action():\n",
    "    action_codes = np.eye(3)\n",
    "    model = FooModel()\n",
    "    print_model(model)\n",
    "    model = model.cuda()\n",
    "    x = torch.randn(4,1)\n",
    "    print('output:')\n",
    "    logits = model(Variable(x).cuda())\n",
    "    probs = F.softmax(logits)\n",
    "    acts = probs.multinomial().detach()\n",
    "    print('probs:', probs, sep='\\n')\n",
    "    print('actions:',acts)\n",
    "    acts_v = action_codes[acts.data.cpu().view(-1).numpy(),:]\n",
    "    print('vectorized actions:', acts_v, 'shape: {0}'.format(acts_v.shape), sep='\\n')\n",
    "    \n",
    "    \n",
    "    model2 = FooModel().cuda()\n",
    "    logits2 = model2(Variable(x).cuda())\n",
    "    log_probs = F.log_softmax(logits2)\n",
    "    print()\n",
    "    print('log_probs:', log_probs, sep='\\n')\n",
    "    log_probs = log_probs.gather(1, acts)\n",
    "    print('selected_log_probs:', log_probs)\n",
    "    \n",
    "    rewards = Variable(torch.ones(log_probs.data.size())).cuda()\n",
    "    print('rewards:', rewards)\n",
    "    actor_loss = torch.neg(torch.mul(log_probs, rewards).sum())\n",
    "    print('actor_loss:', actor_loss)\n",
    "    model2.zero_grad()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    actor_loss.backward()\n",
    "    print('GRADIENTS:')\n",
    "    for name, m in zip(['MODEL SELECTING LOGS:', 'MODEL PROVIDING LOGS:'], [model, model2]):\n",
    "        print(name)\n",
    "        for p in m.parameters():\n",
    "            print('   GRADS:', p.grad)\n",
    "    \n",
    "test_choose_action()\n",
    "#foo = np.array([[0.05, 0.05, 0.9], [0.5, 0.5, 0.0]])\n",
    "#foo = Variable(torch.from_numpy(foo))\n",
    "#for _ in range(20):\n",
    "#    print(foo.multinomial().data.view(-1).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Протестируем сохраниение у загрузку модели с нуля\n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lr_scheduler import LinearAnnealingLR\n",
    "\n",
    "class FooModel(nn.Module):\n",
    "    def __init__(self, constant_init=None):\n",
    "        super(FooModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 3, bias=False)\n",
    "        if constant_init is not None:\n",
    "            nn_init.constant(self.fc1.weight, constant_init)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_model(model):\n",
    "    print('Initial parameters:')\n",
    "    for p in list(model.parameters()):\n",
    "        print(type(p), p.size())\n",
    "        print(p.data.numpy())\n",
    "\n",
    "    \n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar', is_best=False):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'best_checkpoint.pth.tar')\n",
    "\n",
    "\n",
    "def init_foo_model(args):\n",
    "    model = FooModel()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args['inital_lr'], momentum=args['momentum']) \n",
    "    lr_scheduler = LinearAnnealingLR(optimizer,\n",
    "                                     args['total_steps'],\n",
    "                                     end_lr=args['end_lr'],\n",
    "                                     last_step=args['last_step'])\n",
    "    return model, optimizer, lr_scheduler\n",
    "\n",
    "    \n",
    "def load_from_checkpoint(checkpoint_path, restore_optimizer=True):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    args = checkpoint['args']\n",
    "    model = FooModel()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args['inital_lr'], momentum=args['momentum'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if restore_optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_scheduler = LinearAnnealingLR(optimizer,\n",
    "                                     args['total_steps'],\n",
    "                                     end_lr=args['end_lr'],\n",
    "                                     last_step=args['last_step'])\n",
    "    \n",
    "    return model, optimizer, lr_scheduler, args\n",
    "\n",
    "\n",
    "def test_save_load(lr=0.05, total_steps=20, interrupt_step=-1, print_every = 1):\n",
    "    \n",
    "    def L1loss(target, pred):\n",
    "        return  torch.mean(torch.abs(target - pred))\n",
    "    \n",
    "    def save_and_interrupt(args, checkpoint_path):\n",
    "        \n",
    "        nonlocal model, optimizer, lr_scheduler\n",
    "        print(' ============================ TRAINING IS INTERRUPTED! ============================')\n",
    "        print('args:',args, sep='\\n')\n",
    "        print('optimizer:', optimizer.state_dict(), sep='\\n')\n",
    "        print('lr_scheduler:', {'init_lrs':lr_scheduler.init_lrs,\n",
    "                               'end_lrs':lr_scheduler.end_lrs,\n",
    "                               'lr_deltas':lr_scheduler.lr_deltas\n",
    "                              }, sep='\\n')\n",
    "        state = {\n",
    "            'args':args,\n",
    "            'model_state_dict':model.state_dict(),\n",
    "            'optimizer_state_dict':optimizer.state_dict()\n",
    "        }\n",
    "        \n",
    "        save_checkpoint(state, checkpoint_path)\n",
    "        del model, optimizer, lr_scheduler\n",
    "        print(' =========================== /TRAINING IS INTERRUPTED! ============================')\n",
    "    \n",
    "    def restore_from_checkpoint(checkpoint_path, restore_optimizer=False):\n",
    "        print(' =========================== RESTORE FROM CHECKPOINT ============================')\n",
    "        nonlocal args, model, optimizer, lr_scheduler \n",
    "        model, optimizer, lr_scheduler, args = load_from_checkpoint(checkpoint_path, restore_optimizer)\n",
    "        print('args:',args, sep='\\n')\n",
    "        print('optimizer:', optimizer.state_dict(), sep='\\n')\n",
    "        print('lr_scheduler:', {'init_lrs':lr_scheduler.init_lrs,\n",
    "                               'end_lrs':lr_scheduler.end_lrs,\n",
    "                               'lr_deltas':lr_scheduler.lr_deltas\n",
    "                              }, sep='\\n')\n",
    "        print(' =========================== /RESTORE FROM CHECKPOINT ============================')\n",
    "    \n",
    "    torch.manual_seed(14)\n",
    "    checkpoint_path = \"checkpoint.pth.tar\"\n",
    "    args = dict(total_steps=total_steps,\n",
    "                last_step=-1,\n",
    "                inital_lr=lr,\n",
    "                end_lr=0.0,\n",
    "                momentum=0.9)\n",
    "\n",
    "    \n",
    "    inputs = Variable(torch.ones(1,1))\n",
    "    target = Variable(torch.Tensor([[2.,3.,5.]]))\n",
    "    print('target properties:', target.requires_grad, target.volatile)\n",
    "    model, optimizer, lr_scheduler = init_foo_model(args)\n",
    "    print_model(model)\n",
    "    \n",
    "    for t in range(args['last_step']+1, total_steps):\n",
    "        lr_scheduler.adjust_learning_rate()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(inputs)\n",
    "        loss_t = L1loss(target, pred)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        if (t+1) % print_every == 0:\n",
    "            print( '=='*12, 'STEP%d' % (t+1), '='*12)\n",
    "            print('loss:', loss_t.data.numpy()[0], 'opt.lr:', optimizer.param_groups[0]['lr'])\n",
    "            print('pred:', pred.data.numpy()[0])\n",
    "            print('weight:', model.fc1.weight.data.numpy().T[0])\n",
    "            \n",
    "        if t == interrupt_step:\n",
    "            args['last_step'] = t\n",
    "            save_and_interrupt(args, checkpoint_path)\n",
    "            restore_from_checkpoint(checkpoint_path,restore_optimizer=True)\n",
    "            \n",
    "test_save_load(lr=0.05, total_steps=400, interrupt_step=358, print_every=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Values without interuption:<br>\n",
    "======================== STEP360 ============<br>\n",
    "loss: 0.00204961 opt.lr: 0.0051250000000000046<br>\n",
    "pred: [ 1.9987938  2.9992812  4.9957762]<br>\n",
    "weight: [ 1.9986775  2.9971888  4.9959536]<br>\n",
    "======================== STEP400 ============<br>\n",
    "loss: 0.000298937 opt.lr: 0.0001250000000000001<br>\n",
    "pred: [ 1.9994451  2.9996829  4.9999752]<br>\n",
    "weight: [ 1.9995375  2.9997983  5.000052 ]<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Реализуем линейный график уменьшения скорости обучения\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer param_groups:\n",
      "<class 'list'>\n",
      "Initial parameters:\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3, 1])\n",
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 3x1]\n",
      "\n",
      "Initial prediction:\n",
      "Variable containing:\n",
      " 1  1  1\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n",
      "Initial target:\n",
      "Variable containing:\n",
      " 2  3  5\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n",
      "Initial loss:\n",
      "Variable containing:\n",
      " 2.3333\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "======================== STEP40 ============\n",
      "loss: 1.83771 opt.lr: 0.025625000000000002\n",
      "pred: [ 1.4956249  1.4956249  1.4956249]\n",
      "weight: [ 1.5041666  1.5041666  1.5041666]\n",
      "======================== STEP80 ============\n",
      "loss: 1.65854 opt.lr: 0.0006250000000000006\n",
      "pred: [ 1.6747917  1.6747917  1.6747917]\n",
      "weight: [ 1.6750001  1.6750001  1.6750001]\n",
      "======================== STEP120 ============\n",
      "loss: 1.65833 opt.lr: 0.0\n",
      "pred: [ 1.6750001  1.6750001  1.6750001]\n",
      "weight: [ 1.6750001  1.6750001  1.6750001]\n",
      "======================== STEP160 ============\n",
      "loss: 1.65833 opt.lr: 0.0\n",
      "pred: [ 1.6750001  1.6750001  1.6750001]\n",
      "weight: [ 1.6750001  1.6750001  1.6750001]\n",
      "======================== STEP200 ============\n",
      "loss: 1.65833 opt.lr: 0.0\n",
      "pred: [ 1.6750001  1.6750001  1.6750001]\n",
      "weight: [ 1.6750001  1.6750001  1.6750001]\n",
      "======================== STEP240 ============\n",
      "loss: 1.65833 opt.lr: 0.0\n",
      "pred: [ 1.6750001  1.6750001  1.6750001]\n",
      "weight: [ 1.6750001  1.6750001  1.6750001]\n",
      "======================== STEP280 ============\n",
      "loss: 1.65833 opt.lr: 0.0\n",
      "pred: [ 1.6750001  1.6750001  1.6750001]\n",
      "weight: [ 1.6750001  1.6750001  1.6750001]\n",
      "======================== STEP320 ============\n",
      "loss: 1.65833 opt.lr: 0.0\n",
      "pred: [ 1.6750001  1.6750001  1.6750001]\n",
      "weight: [ 1.6750001  1.6750001  1.6750001]\n",
      "======================== STEP360 ============\n",
      "loss: 1.65833 opt.lr: 0.0\n",
      "pred: [ 1.6750001  1.6750001  1.6750001]\n",
      "weight: [ 1.6750001  1.6750001  1.6750001]\n",
      "======================== STEP400 ============\n",
      "loss: 1.65833 opt.lr: 0.0\n",
      "pred: [ 1.6750001  1.6750001  1.6750001]\n",
      "weight: [ 1.6750001  1.6750001  1.6750001]\n"
     ]
    }
   ],
   "source": [
    "def test_annealing_with_model(lr=0.05, num_steps=20, print_every = 1):\n",
    "    from lr_scheduler import LinearAnnealingLR\n",
    "    def L1loss(target, pred):\n",
    "        return  torch.mean(torch.abs(target - pred))\n",
    "\n",
    "    foo = FooModel(constant_init=1.)\n",
    "    optimizer = optim.SGD(foo.parameters(), lr=lr, momentum=0.0)\n",
    "    lr_scheduler = LinearAnnealingLR(optimizer, 80)\n",
    "    \n",
    "    x = Variable(torch.ones(1,1))\n",
    "    target = Variable(torch.Tensor([[2.,3.,5.]]))\n",
    "    pred = foo(x)\n",
    "    loss0 = L1loss(target, pred)\n",
    "    \n",
    "    print('Optimizer param_groups:', type(optimizer.param_groups), sep='\\n')\n",
    "    print('Initial parameters:')\n",
    "    for p in list(foo.parameters()):\n",
    "        print(type(p), p.size())\n",
    "        print(p.data)\n",
    "    \n",
    "    print('Initial prediction:',pred, sep='\\n')\n",
    "    print('Initial target:',target, sep='\\n')\n",
    "    print('Initial loss:', loss0, sep='\\n')\n",
    "    \n",
    "    for t in range(num_steps):\n",
    "        lr_scheduler.adjust_learning_rate()\n",
    "        optimizer.zero_grad()\n",
    "        pred = foo(x)\n",
    "        loss_t = L1loss(target, pred)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        if (t+1) % print_every == 0:\n",
    "            print( '=='*12, 'STEP%d' % (t+1), '='*12)\n",
    "            print('loss:', loss_t.data.numpy()[0], 'opt.lr:', optimizer.param_groups[0]['lr'])\n",
    "            print('pred:', pred.data.numpy()[0])\n",
    "            print('weight:', foo.fc1.weight.data.numpy().T[0])\n",
    "\n",
    "%precision 7\n",
    "test_annealing_with_model(lr=0.05, num_steps=400, print_every=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "FFNetwork (\n",
      "  (conv1): Conv2d(4, 16, kernel_size=(8, 8), stride=(4, 4))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2))\n",
      "  (fc3): Linear (2592 -> 256)\n",
      "  (fc_policy): Linear (256 -> 6)\n",
      "  (fc_value): Linear (256 -> 1)\n",
      ")\n",
      "optimizer BEFORE:\n",
      "group #0 lr: 1.0 initial_rl: None\n",
      "optimizer AFTER:\n",
      "group #0 lr: 1.0 initial_rl: 1.0\n",
      "\n",
      "\n",
      "\n",
      "STEP #0 ========================================\n",
      "group #0 lr: 1.0 initial_rl: 1.0\n",
      "STEP #1 ========================================\n",
      "group #0 lr: 0.95 initial_rl: 1.0\n",
      "STEP #2 ========================================\n",
      "group #0 lr: 0.9 initial_rl: 1.0\n",
      "STEP #3 ========================================\n",
      "group #0 lr: 0.85 initial_rl: 1.0\n",
      "STEP #4 ========================================\n",
      "group #0 lr: 0.8 initial_rl: 1.0\n",
      "INTERUPTED!\n",
      "...\n",
      "LEARNING RESUMED:\n",
      "STEP #5 ========================================\n",
      "group #0 lr: 0.75 initial_rl: 1.0\n",
      "STEP #6 ========================================\n",
      "group #0 lr: 0.7 initial_rl: 1.0\n",
      "STEP #7 ========================================\n",
      "group #0 lr: 0.6499999999999999 initial_rl: 1.0\n",
      "STEP #8 ========================================\n",
      "group #0 lr: 0.6 initial_rl: 1.0\n",
      "STEP #9 ========================================\n",
      "group #0 lr: 0.55 initial_rl: 1.0\n",
      "STEP #10 ========================================\n",
      "group #0 lr: 0.5 initial_rl: 1.0\n",
      "STEP #11 ========================================\n",
      "group #0 lr: 0.44999999999999996 initial_rl: 1.0\n",
      "STEP #12 ========================================\n",
      "group #0 lr: 0.3999999999999999 initial_rl: 1.0\n",
      "STEP #13 ========================================\n",
      "group #0 lr: 0.35 initial_rl: 1.0\n",
      "STEP #14 ========================================\n",
      "group #0 lr: 0.29999999999999993 initial_rl: 1.0\n",
      "STEP #15 ========================================\n",
      "group #0 lr: 0.25 initial_rl: 1.0\n",
      "STEP #16 ========================================\n",
      "group #0 lr: 0.19999999999999996 initial_rl: 1.0\n",
      "STEP #17 ========================================\n",
      "group #0 lr: 0.1499999999999999 initial_rl: 1.0\n",
      "STEP #18 ========================================\n",
      "group #0 lr: 0.09999999999999998 initial_rl: 1.0\n",
      "STEP #19 ========================================\n",
      "group #0 lr: 0.04999999999999993 initial_rl: 1.0\n",
      "STEP #20 ========================================\n",
      "group #0 lr: 0.0 initial_rl: 1.0\n"
     ]
    }
   ],
   "source": [
    "def test_linear_annealing(lr=1., model=None, interrupt_step=None, final_step=20):\n",
    "    \n",
    "    def print_opt(opt):\n",
    "        for i, p in enumerate(opt.param_groups):\n",
    "            print('group #%d'%i, 'lr:', p['lr'], 'initial_rl:', p.get('initial_lr', None))\n",
    "    \n",
    "    def train(opt, lr_scheduler, init_step, final_step):\n",
    "        for i in range(init_step, final_step+1):\n",
    "            print('STEP #%d'%i, '=='*20)\n",
    "            lr_scheduler.adjust_learning_rate()\n",
    "            print_opt(opt)\n",
    "    \n",
    "    if model is None:\n",
    "        model = FooModel()\n",
    "    print('Model:', model, sep='\\n')\n",
    "    opt = optim.SGD(model.parameters(), lr=1., momentum=0.)\n",
    "    print('optimizer BEFORE:')\n",
    "    print_opt(opt)\n",
    "    \n",
    "    from lr_scheduler import LinearAnnealingLR\n",
    "    lr_scheduler = LinearAnnealingLR(opt, final_step)\n",
    "    print('optimizer AFTER:')\n",
    "    print_opt(opt)\n",
    "    print('\\n\\n')\n",
    "    if interrupt_step is not None and 0 <= interrupt_step < final_step:\n",
    "        train(opt, lr_scheduler, 0, interrupt_step)\n",
    "        print('LEARNING INTERUPTED!')\n",
    "        print('...')\n",
    "        print('LEARNING RESUMED:')\n",
    "        lr_scheduler = LinearAnnealingLR(opt, final_step, last_step=interrupt_step)\n",
    "        train(opt, lr_scheduler, interrupt_step+1, final_step)\n",
    "    else:\n",
    "        train(opt, lr_scheduler, 0, final_step) \n",
    "    \n",
    "%precision 3   \n",
    "test_linear_annealing(1., model, interrupt_step=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== foo() =============\n",
      "z: 17\n",
      "========== apply_fun() =============\n",
      "z: 7\n",
      "========== print_z() =============\n",
      "10 11 12 13 \n",
      "========== /print_z() ============\n",
      "z: 7\n",
      "========== /apply_fun() ============\n",
      "z: 14\n",
      "========== /foo() =============\n"
     ]
    }
   ],
   "source": [
    "def foo():\n",
    "    \n",
    "    def apply_fun(fun, *args, **kwargs):\n",
    "        z = 7\n",
    "        print('========== apply_fun() =============')\n",
    "        print('z:', z)\n",
    "        fun(*args, **kwargs)\n",
    "        print('z:', z)\n",
    "        print('========== /apply_fun() ============')\n",
    "    \n",
    "    def print_z(n):\n",
    "        print('========== print_z() =============')\n",
    "        nonlocal z\n",
    "        z = 10\n",
    "        for i in range(n):\n",
    "            print(z, end=' ')\n",
    "            z+=1\n",
    "        print('\\n========== /print_z() ============')\n",
    "        \n",
    "    z = 17\n",
    "    print('========== foo() =============')\n",
    "    print('z:', z)\n",
    "    del z\n",
    "    apply_fun(print_z, 4)\n",
    "    print('z:', z)\n",
    "    print('========== /foo() =============')\n",
    "    \n",
    "    \n",
    "foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints/chkpt_100.pth is file: True\n",
      "./checkpoints/chkpt_200.pth is file: True\n",
      "./checkpoints/chkpt_300.pth is file: False\n",
      "./checkpoints/chkpt_last.pth is file: True\n",
      "./checkpoints/chkpt_best.pth is file: True\n"
     ]
    }
   ],
   "source": [
    "from os.path import join as join_path\n",
    "import os\n",
    "\n",
    "def test_file_loading():\n",
    "    CHECKPOINT_SUBDIR = 'checkpoints'\n",
    "    CHECKPOINT_PATTERN = 'chkpt_{}.pth'\n",
    "    #CHECKPOINT_PATTERN = 'chkpt_last.pth'\n",
    "    CHECKPOINT_BEST = 'chkpt_best.pth'\n",
    "    df = '.'\n",
    "    for sfx in [100,200,300, 'last', 'best']:\n",
    "        filename = join_path(df, CHECKPOINT_SUBDIR, CHECKPOINT_PATTERN).format(sfx)\n",
    "        print(filename, end=' ')\n",
    "        print('is file:', os.path.isfile(filename))\n",
    "    \n",
    "test_file_loading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5673395  0.0287094 -0.2457747]\n",
      " [ 0.6237779  0.6835583 -0.1865216]] torch.Size([2, 3]) 1.1288244219204344\n",
      "[[-0.8743114 -0.0722197 -0.3958555]\n",
      " [ 0.8653257 -1.2069381 -0.3034301]\n",
      " [ 0.2716988  1.4879425  0.4299813]] torch.Size([3, 3]) 2.3867493890189047\n",
      "[[-1.1603181 -0.4130548  0.6309335]\n",
      " [-2.5268936 -0.6300246  0.5726606]\n",
      " [ 0.0211443 -0.3264207 -0.6226609]\n",
      " [ 0.2076057  0.8090831  0.8390475]] torch.Size([4, 3]) 3.304771160800798\n",
      "Computed total_norm: 4.2299325818228795\n",
      "Correct total norm: 4.2299325818228795\n",
      "Norms are equal: True\n"
     ]
    }
   ],
   "source": [
    "def test_total_norm(norm_type=2):\n",
    "    all_params = []\n",
    "    total_norm = 0.\n",
    "    for i in range(3):\n",
    "        foo = torch.randn(i+2, 3)\n",
    "        local_norm = foo.norm(norm_type)\n",
    "        print(foo.numpy(), foo.size(), local_norm)\n",
    "        all_params.append(foo)\n",
    "\n",
    "        total_norm += local_norm**norm_type\n",
    "    total_norm = total_norm ** (1./norm_type)\n",
    "    print('Computed total_norm:', total_norm)\n",
    "    \n",
    "    correct_norm = torch.cat(map(lambda x: x.view(-1), all_params)).norm(norm_type)\n",
    "    print('Correct total norm:', correct_norm)\n",
    "    print('Norms are equal:', correct_norm == total_norm)\n",
    "test_total_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      "\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "after:\n",
      "\n",
      " 3\n",
      " 0\n",
      " 3\n",
      " 0\n",
      " 3\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo2 = torch.ones(3)*3\n",
    "foo1 = torch.zeros(5)\n",
    "\n",
    "foo_mask = torch.ByteTensor([i%2 == 0 for i in range(5)])\n",
    "\n",
    "print('before:', foo1, sep='\\n')\n",
    "foo1[foo_mask] = foo2\n",
    "print('after:', foo1, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  0   1   2   3   4   5\n",
      "  6   7   8   9  10  11\n",
      " 12  13  14  15  16  17\n",
      "[torch.FloatTensor of size 3x6]\n",
      "\n",
      "[[ 0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " 3\n",
      " 4\n",
      " 5\n",
      "[torch.LongTensor of size 3x1]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-29849a08a1c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0macts_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macts_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macts_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mfoo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macts_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfoo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, dim, index)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mGather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, index)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "a = Variable(torch.arange(0,18).view(n,6), requires_grad=True)\n",
    "print(a)\n",
    "acts = np.zeros((n,6))\n",
    "for i in range(n):\n",
    "    acts[i,3+i] = 1\n",
    "print(acts)\n",
    "\n",
    "acts_ids = np.argmax(acts, 1)\n",
    "acts_ids = torch.from_numpy(acts_ids).view(n,-1)\n",
    "print(acts_ids)\n",
    "foo = a.gather(i, Variable(acts_ids))\n",
    "print(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared_done:\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 1.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "shared_rewards:\n",
      "[[ 1.3  0.   0.   0. ]\n",
      " [ 0.   1.7  0.   0. ]\n",
      " [ 0.   0.   1.3  0. ]\n",
      " [ 0.   0.   0.   1.6]\n",
      " [ 1.3  0.   0.   0. ]\n",
      " [ 0.   1.7  0.   0. ]]\n",
      "================================================================================\n",
      "function: old_way\n",
      "total_episode_rewards:\n",
      "[ 1.3  1.7  0.   1.6]\n",
      "emulator_steps:\n",
      "[2 1 2 3]\n",
      "rewards:\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]]\n",
      "global_steps: 24\n",
      "total_rewards: [1.3, 0.0, 0.0, 1.3, 1.7]\n",
      "================================================================================\n",
      "function: new_way\n",
      "total_episode_rewards:\n",
      "[ 1.3  1.7  0.   1.6]\n",
      "emulator_steps:\n",
      "[2 1 2 3]\n",
      "rewards:\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]]\n",
      "global_steps: 24\n",
      "total_rewards: [1.3, 0.0, 0.0, 1.3, 1.7]\n"
     ]
    }
   ],
   "source": [
    "def vectorize_shared_r_and_shared_done_handling():\n",
    "    num_emulators = 4\n",
    "    num_actions = 3\n",
    "    max_local_steps = 6\n",
    "    shared_rewards = np.zeros((max_local_steps, num_emulators))\n",
    "    for i, row in enumerate(shared_rewards):\n",
    "        row[i%num_emulators] = 1. + np.random.randint(1,11)/10.\n",
    "    \n",
    "    shared_done = np.zeros((max_local_steps, num_emulators))\n",
    "    done_times = [(0,0), (3,0), (4,1), (3,2), (2,3)]\n",
    "    done_t, done_emulator = zip(*done_times) \n",
    "    shared_done[done_t, done_emulator] = 1\n",
    "    get_actions = lambda: np.array([np.random.multinomial(1,np.full(num_actions, 1./num_actions)) for i in range(num_emulators)])\n",
    "    \n",
    "    print('shared_done:', shared_done, sep='\\n')\n",
    "    print('shared_rewards:', shared_rewards, sep='\\n')\n",
    "    \n",
    "    def old_way(t, shared_r, shared_done, actions):\n",
    "        nonlocal global_step, total_episode_rewards, rewards \n",
    "        nonlocal total_rewards, emulator_steps, actions_sum \n",
    "        actions_sum += actions \n",
    "        for e, (actual_reward, episode_over) in enumerate(zip(shared_r, shared_done)):\n",
    "            total_episode_rewards[e] += actual_reward\n",
    "            actual_reward = np.clip(actual_reward, -1.,1.)\n",
    "            rewards[t, e] = actual_reward\n",
    "\n",
    "            emulator_steps[e] += 1\n",
    "            global_step += 1\n",
    "            if episode_over:\n",
    "                total_rewards.append(total_episode_rewards[e])\n",
    "                total_episode_rewards[e] = 0\n",
    "                emulator_steps[e] = 0\n",
    "                actions_sum[e] = np.zeros(num_actions)\n",
    "    \n",
    "    def new_way(t, shared_r, shared_done, num_actions):\n",
    "        nonlocal global_step, total_episode_rewards, rewards \n",
    "        nonlocal total_rewards, emulator_steps, actions_sum \n",
    "        done_mask = shared_done.astype(bool) \n",
    "        total_episode_rewards += shared_r\n",
    "        rewards[t] = np.clip(shared_r, -1., 1.)\n",
    "        emulator_steps += 1\n",
    "        global_step += num_emulators\n",
    "        total_rewards.extend(total_episode_rewards[done_mask])\n",
    "        total_episode_rewards[done_mask] = 0.\n",
    "        emulator_steps[done_mask] = 0\n",
    "        actions_sum[done_mask,:] = 0\n",
    "        \n",
    "    for f in [old_way, new_way]:\n",
    "        print('=='*40)\n",
    "        print('function:', f.__name__)\n",
    "        global_step = 0\n",
    "        total_episode_rewards = np.zeros(num_emulators)\n",
    "        emulator_steps = np.zeros(num_emulators, dtype=np.int64)\n",
    "        actions_sum = np.zeros((num_emulators, num_actions))\n",
    "        rewards = np.zeros((max_local_steps, num_emulators))\n",
    "        total_rewards = []\n",
    "        \n",
    "        for t in range(max_local_steps):\n",
    "            actions_t = get_actions()\n",
    "            #print('STEP#%d'%t)\n",
    "            #print('actions_t:',actions_t, sep='\\n')\n",
    "            f(t, shared_rewards[t], shared_done[t], actions_t)\n",
    "        \n",
    "        print('total_episode_rewards:', total_episode_rewards, sep='\\n')\n",
    "        print('emulator_steps:', emulator_steps, sep='\\n')\n",
    "        print('rewards:', rewards, sep='\\n')\n",
    "        print('global_steps:', global_step)\n",
    "        print('total_rewards:', total_rewards)\n",
    "        \n",
    "        \n",
    "vectorize_shared_r_and_shared_done_handling()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 3.23 ms per loop\n",
      "1000 loops, best of 3: 1.91 ms per loop\n"
     ]
    }
   ],
   "source": [
    "def compare_handling(use_new=True):\n",
    "    num_emulators = 4\n",
    "    num_actions = 3\n",
    "    max_local_steps = 6\n",
    "    shared_rewards = np.zeros((max_local_steps, num_emulators))\n",
    "    for i, row in enumerate(shared_rewards):\n",
    "        row[i%num_emulators] = 1. + np.random.randint(1,11)/10.\n",
    "    \n",
    "    shared_done = np.zeros((max_local_steps, num_emulators))\n",
    "    done_times = [(0,0), (3,0), (4,1), (3,2), (2,3)]\n",
    "    done_t, done_emulator = zip(*done_times) \n",
    "    shared_done[done_t, done_emulator] = 1\n",
    "    get_actions = lambda: np.array([np.random.multinomial(1,np.full(num_actions, 1./num_actions)) for i in range(num_emulators)])\n",
    "\n",
    "    \n",
    "    def old_way(t, shared_r, shared_done, actions):\n",
    "        nonlocal global_step, total_episode_rewards, rewards \n",
    "        nonlocal total_rewards, emulator_steps, actions_sum \n",
    "        actions_sum += actions \n",
    "        for e, (actual_reward, episode_over) in enumerate(zip(shared_r, shared_done)):\n",
    "            total_episode_rewards[e] += actual_reward\n",
    "            actual_reward = np.clip(actual_reward, -1.,1.)\n",
    "            rewards[t, e] = actual_reward\n",
    "\n",
    "            emulator_steps[e] += 1\n",
    "            global_step += 1\n",
    "            if episode_over:\n",
    "                total_rewards.append(total_episode_rewards[e])\n",
    "                total_episode_rewards[e] = 0\n",
    "                emulator_steps[e] = 0\n",
    "                actions_sum[e] = np.zeros(num_actions)\n",
    "    \n",
    "    def new_way(t, shared_r, shared_done, num_actions):\n",
    "        nonlocal global_step, total_episode_rewards, rewards \n",
    "        nonlocal total_rewards, emulator_steps, actions_sum \n",
    "        done_mask = shared_done.astype(bool) \n",
    "        total_episode_rewards += shared_r\n",
    "        rewards[t] = np.clip(shared_r, -1., 1.)\n",
    "        emulator_steps += 1\n",
    "        global_step += num_emulators\n",
    "        total_rewards.extend(total_episode_rewards[done_mask])\n",
    "        total_episode_rewards[done_mask] = 0.\n",
    "        emulator_steps[done_mask] = 0\n",
    "        actions_sum[done_mask,:] = 0\n",
    "        \n",
    "\n",
    "    global_step = 0\n",
    "    total_episode_rewards = np.zeros(num_emulators)\n",
    "    emulator_steps = np.zeros(num_emulators, dtype=np.int64)\n",
    "    actions_sum = np.zeros((num_emulators, num_actions))\n",
    "    rewards = np.zeros((max_local_steps, num_emulators))\n",
    "    total_rewards = []\n",
    "    actions_t = get_actions()\n",
    "    fun =  new_way if use_new else old_way    \n",
    "    t = 0    \n",
    "    for _ in range(200):\n",
    "        fun(t, shared_rewards[t], shared_done[t], actions_t)\n",
    "                \n",
    "%timeit -n1000 compare_handling(use_new=False)\n",
    "%timeit -n1000 compare_handling(use_new=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo:\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 2.]]\n",
      "foo:\n",
      "[[ 3.]\n",
      " [ 4.]\n",
      " [ 5.]]\n",
      "foo:\n",
      "[[ 6.]\n",
      " [ 7.]\n",
      " [ 8.]]\n",
      "foo:\n",
      "[[  9.]\n",
      " [ 10.]\n",
      " [ 11.]]\n",
      "foo:\n",
      "[[ 12.]\n",
      " [ 13.]\n",
      " [ 14.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "    0\n",
       "    1\n",
       "    2\n",
       "    3\n",
       "    4\n",
       "    5\n",
       "    6\n",
       "    7\n",
       "    8\n",
       "    9\n",
       "   10\n",
       "   11\n",
       "   12\n",
       "   13\n",
       "   14\n",
       "[torch.FloatTensor of size 15x1]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foos = []\n",
    "first = 0\n",
    "for i in range(5):\n",
    "    foo = torch.arange(first, first + 3).view(3,1)\n",
    "    print('foo:', foo.numpy(), sep='\\n')\n",
    "    foos.append(foo)\n",
    "    first += 3\n",
    "torch.cat(foos, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "plus:\n",
      "\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      "[torch.FloatTensor of size 4]\n",
      "\n",
      "pow^2:\n",
      "\n",
      "  9\n",
      " 16\n",
      " 25\n",
      " 36\n",
      "[torch.FloatTensor of size 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo = torch.ones(4)*2\n",
    "foo2 = torch.arange(1,5)\n",
    "#print(foo)\n",
    "#print(foo2)\n",
    "print(torch.equal(foo * foo2, torch.mul(foo, foo2)))\n",
    "print('plus:')\n",
    "print(foo2 + foo)\n",
    "print('pow^2:', (foo2 + foo).pow(2), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25  0.25  0.25  0.25] entropy: 1.38629436112\n",
      "[ 0.  1.  0.  0.] entropy: -2.22044604925e-16\n"
     ]
    }
   ],
   "source": [
    "uniform = np.full(4, 0.25)\n",
    "determ = np.zeros(4)\n",
    "determ[1] = 1.\n",
    "eps = np.finfo(np.float64).eps\n",
    "entropy = lambda p: -np.dot(np.log(p+eps) , p)\n",
    "print(uniform, 'entropy:', entropy(uniform))\n",
    "print(determ, 'entropy:', entropy(determ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.428009932726  -- increase prob --> -0.267772261577\n",
      "0.428009932726  -- decrease prob --> 0.831776616672\n"
     ]
    }
   ],
   "source": [
    "print(np.log(.7)*1.2,' -- increase prob -->', np.log(.8)*1.2)\n",
    "print(np.log(.7)*-1.2,' -- decrease prob -->', np.log(.5)*-1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%pdoc np.finfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
