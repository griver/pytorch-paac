{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, sys, time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "Variable containing:\n",
      "-0.2000 -0.0000 -1.0000  1.2000  0.6000 -1.2000  0.0000  0.4000  1.4000  0.4000\n",
      " 0.4000  1.0000 -2.2000 -0.0000  0.2000 -0.4000 -0.4000  1.4000  0.8000 -2.2000\n",
      "-1.4000 -1.0000 -0.2000 -1.0000  0.2000 -0.8000 -0.2000 -0.6000 -1.4000  0.6000\n",
      "-0.4000 -2.0000  0.4000  0.2000 -1.6000 -1.0000  0.4000  1.2000 -1.8000 -1.0000\n",
      " 0.2000  0.6000  1.4000  2.0000 -0.2000 -0.2000  1.6000 -1.2000  0.2000  0.8000\n",
      " 1.2000 -0.8000  0.8000  0.2000 -0.4000 -0.6000 -0.4000 -1.0000 -1.8000 -0.2000\n",
      " 0.0000 -1.2000 -0.4000 -0.6000 -1.0000  0.2000 -0.8000  1.0000 -1.4000  0.2000\n",
      " 1.0000  1.8000  1.0000  1.6000  0.6000 -0.8000 -0.4000  0.4000  0.4000  1.2000\n",
      "-0.0000  1.8000 -0.6000 -1.6000 -2.0000 -0.6000  0.4000 -0.8000 -0.2000 -2.0000\n",
      "-1.0000  1.4000  1.6000  0.0000  0.2000 -1.0000 -0.2000 -0.2000  1.2000 -1.6000\n",
      "[torch.FloatTensor of size 10x10]\n",
      "\n",
      "targets\n",
      "Variable containing:\n",
      " 3\n",
      " 2\n",
      " 1\n",
      " 0\n",
      " 2\n",
      " 2\n",
      " 0\n",
      " 0\n",
      " 3\n",
      " 3\n",
      "[torch.LongTensor of size 10]\n",
      "\n",
      "======================================\n",
      "0 epochs have been finished\n",
      "Loss: 1.43478262424469 accuracy: 0.1\n",
      "Targets: [3 2 1 0 2 2 0 0 3 3]\n",
      "Preds:   [0 2 3 2 0 3 3 2 2 2]\n",
      "======================================\n",
      "1 epochs have been finished\n",
      "Loss: 0.9864872694015503 accuracy: 0.8\n",
      "Targets: [3 2 1 0 2 2 0 0 3 3]\n",
      "Preds:   [0 2 0 0 2 2 0 0 3 3]\n",
      "======================================\n",
      "2 epochs have been finished\n",
      "Loss: 0.7993439435958862 accuracy: 0.9\n",
      "Targets: [3 2 1 0 2 2 0 0 3 3]\n",
      "Preds:   [0 2 1 0 2 2 0 0 3 3]\n",
      "======================================\n",
      "3 epochs have been finished\n",
      "Loss: 0.6790451407432556 accuracy: 1.0\n",
      "Targets: [3 2 1 0 2 2 0 0 3 3]\n",
      "Preds:   [3 2 1 0 2 2 0 0 3 3]\n",
      "======================================\n",
      "4 epochs have been finished\n",
      "Loss: 0.5861141681671143 accuracy: 1.0\n",
      "Targets: [3 2 1 0 2 2 0 0 3 3]\n",
      "Preds:   [3 2 1 0 2 2 0 0 3 3]\n",
      "======================================\n",
      "5 epochs have been finished\n",
      "Loss: 0.515900194644928 accuracy: 1.0\n",
      "Targets: [3 2 1 0 2 2 0 0 3 3]\n",
      "Preds:   [3 2 1 0 2 2 0 0 3 3]\n",
      "======================================\n",
      "6 epochs have been finished\n",
      "Loss: 0.46063995361328125 accuracy: 1.0\n",
      "Targets: [3 2 1 0 2 2 0 0 3 3]\n",
      "Preds:   [3 2 1 0 2 2 0 0 3 3]\n"
     ]
    }
   ],
   "source": [
    "def hash(z):\n",
    "    h = 0\n",
    "    for e in z:\n",
    "        h = 10*h + e \n",
    "    return h\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.fc1(inputs))\n",
    "        y_logits = self.fc2(x)\n",
    "        return y_logits\n",
    "\n",
    "def print_eval(model, X, y, n_epochs):\n",
    "    print('======================================')\n",
    "    print('{} epochs have been finished'.format(n_epochs))    \n",
    "    logits = model(X)\n",
    "    \n",
    "    targets = y.data.numpy()\n",
    "    _, preds = F.softmax(logits).data.max(1)\n",
    "    preds = preds.numpy()\n",
    "    \n",
    "    print('Loss:', cross_entropy(F.log_softmax(logits), y).data[0], end=' ')\n",
    "    print('accuracy:', (targets == preds).mean())\n",
    "    print('Targets:', targets)\n",
    "    print('Preds:  ', preds)\n",
    "\n",
    "\n",
    "input_dim = 10\n",
    "H = 20\n",
    "output_dim = 4\n",
    "\n",
    "N = 10\n",
    "\n",
    "X = (th.randn(N,input_dim)*5).round().div(5)\n",
    "X = Variable(X)\n",
    "y = Variable(th.LongTensor(N).random_(output_dim)) #requires_grad is False by default \n",
    "\n",
    "print('inputs')\n",
    "print(X)\n",
    "print('targets')\n",
    "print(y)\n",
    "\n",
    "model = MyNetwork(input_dim, H, output_dim)\n",
    "initial_params = [(name, p.data.clone()) for name, p in model.named_parameters()]\n",
    "first_layer_params = [p for name, p in model.named_parameters() if 'fc1' in name]\n",
    "\n",
    "\n",
    "cross_entropy = nn.NLLLoss()\n",
    "optimizer = optim.SGD(first_layer_params, lr=1e-3, momentum=0.9)\n",
    "\n",
    "n_steps = 500\n",
    "n_epochs = 6 \n",
    "for epoch in range(n_epochs):\n",
    "    print_eval(model, X, y, epoch)\n",
    "    for i in range(n_steps):\n",
    "        log_preds = F.log_softmax(model(X))\n",
    "        loss = cross_entropy(log_preds, y)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print_eval(model, X, y, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if params are the same:\n",
      "==========================\n",
      "fc1.weight shapes: torch.Size([20, 10]) torch.Size([20, 10])\n",
      "params are the same: False\n",
      "==========================\n",
      "fc1.bias shapes: torch.Size([20]) torch.Size([20])\n",
      "params are the same: False\n",
      "==========================\n",
      "fc2.weight shapes: torch.Size([4, 20]) torch.Size([4, 20])\n",
      "params are the same: True\n",
      "==========================\n",
      "fc2.bias shapes: torch.Size([4]) torch.Size([4])\n",
      "params are the same: True\n"
     ]
    }
   ],
   "source": [
    "print('Check if params are the same:')\n",
    "trained_params = list(model.named_parameters())\n",
    "\n",
    "for i in range(len(trained_params)):\n",
    "    name_init, p_init = initial_params[i]\n",
    "    name_new, p_new = trained_params[i]\n",
    "    assert name_init == name_new, 'Мы сравниваем только одинаковые параметры!'\n",
    "    print('==========================')\n",
    "    print(name_new, 'shapes:', p_init.size(), p_new.size())\n",
    "    print('params are the same:', th.equal(p_init, p_new.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "in-place operations can be only used on variables that don't share storage with any other variables, but detected that there are 2 objects sharing it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e54c47bcb318>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfoo1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfoo1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'foo1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfoo1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mMaskedFill\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mSetItem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__deepcopy__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: in-place operations can be only used on variables that don't share storage with any other variables, but detected that there are 2 objects sharing it"
     ]
    }
   ],
   "source": [
    "foo = Variable(th.zeros(3,3))\n",
    "\n",
    "foo1 = foo.detach()\n",
    "foo1[0,0] = 1.\n",
    "print('foo1')\n",
    "print(foo1)\n",
    "print('foo')\n",
    "print(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step33\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "for i in range(40):\n",
    "    print('step%d' % i)\n",
    "    sys.stdout.flush()\n",
    "    time.sleep(4.)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import _pickle as pkl\n",
    "from collections import namedtuple\n",
    "\n",
    "TrainingStats = namedtuple('TrainingStats', \"mean_r max_r min_r std_r targets_ratio preds_ration acc prec rec\")\n",
    "FooStats = namedtuple('FooStats', \"epoch iter sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dump(N, M, filename='test'):\n",
    "    \n",
    "    \n",
    "    for epoch in range(N):\n",
    "        l = []\n",
    "        for t in range(M):\n",
    "            l.append(FooStats(epoch, t, epoch + t))\n",
    "            \n",
    "        with open(filename +'.pickle', 'ba') as file:\n",
    "            pkl.dump(l, file, protocol=4)\n",
    "        \n",
    "        \n",
    "test_dump(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-3b0af2d74501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-3b0af2d74501>\u001b[0m in \u001b[0;36mtest_load\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test.pickle'"
     ]
    }
   ],
   "source": [
    "def test_load(filename):\n",
    "    data = []\n",
    "    with open(filename, 'rb') as file:\n",
    "        while True:\n",
    "            try:\n",
    "                data.extend(pkl.load(file))\n",
    "            except EOFError:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "data = test_load('test.pickle')\n",
    "for row in data:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1 SAVE_CHECKPOINT: step=1000000 counter=3125\n",
      "#2 SAVE_CHECKPOINT: step=2000000 counter=6250\n",
      "#3 SAVE_CHECKPOINT: step=3000000 counter=9375\n",
      "#4 SAVE_CHECKPOINT: step=4000000 counter=12500\n",
      "#5 SAVE_CHECKPOINT: step=5000000 counter=15625\n",
      "#6 SAVE_CHECKPOINT: step=6000000 counter=18750\n",
      "#7 SAVE_CHECKPOINT: step=7000000 counter=21875\n",
      "#8 SAVE_CHECKPOINT: step=8000000 counter=25000\n",
      "#9 SAVE_CHECKPOINT: step=9000000 counter=28125\n",
      "#10 SAVE_CHECKPOINT: step=10000000 counter=31250\n",
      "#11 SAVE_CHECKPOINT: step=11000000 counter=34375\n",
      "#12 SAVE_CHECKPOINT: step=12000000 counter=37500\n",
      "#13 SAVE_CHECKPOINT: step=13000000 counter=40625\n",
      "#14 SAVE_CHECKPOINT: step=14000000 counter=43750\n",
      "#15 SAVE_CHECKPOINT: step=15000000 counter=46875\n",
      "#16 SAVE_CHECKPOINT: step=16000000 counter=50000\n",
      "#17 SAVE_CHECKPOINT: step=17000000 counter=53125\n",
      "#18 SAVE_CHECKPOINT: step=18000000 counter=56250\n",
      "#19 SAVE_CHECKPOINT: step=19000000 counter=59375\n",
      "#20 SAVE_CHECKPOINT: step=20000000 counter=62500\n",
      "#21 SAVE_CHECKPOINT: step=21000000 counter=65625\n",
      "#22 SAVE_CHECKPOINT: step=22000000 counter=68750\n",
      "#23 SAVE_CHECKPOINT: step=23000000 counter=71875\n",
      "#24 SAVE_CHECKPOINT: step=24000000 counter=75000\n",
      "#25 SAVE_CHECKPOINT: step=25000000 counter=78125\n",
      "#26 SAVE_CHECKPOINT: step=26000000 counter=81250\n",
      "#27 SAVE_CHECKPOINT: step=27000000 counter=84375\n",
      "#28 SAVE_CHECKPOINT: step=28000000 counter=87500\n",
      "#29 SAVE_CHECKPOINT: step=29000000 counter=90625\n",
      "#30 SAVE_CHECKPOINT: step=30000000 counter=93750\n",
      "#31 SAVE_CHECKPOINT: step=31000000 counter=96875\n",
      "#32 SAVE_CHECKPOINT: step=32000000 counter=100000\n",
      "#33 SAVE_CHECKPOINT: step=33000000 counter=103125\n",
      "#34 SAVE_CHECKPOINT: step=34000000 counter=106250\n",
      "#35 SAVE_CHECKPOINT: step=35000000 counter=109375\n",
      "#36 SAVE_CHECKPOINT: step=36000000 counter=112500\n",
      "#37 SAVE_CHECKPOINT: step=37000000 counter=115625\n",
      "#38 SAVE_CHECKPOINT: step=38000000 counter=118750\n",
      "#39 SAVE_CHECKPOINT: step=39000000 counter=121875\n",
      "#40 SAVE_CHECKPOINT: step=40000000 counter=125000\n",
      "#41 SAVE_CHECKPOINT: step=41000000 counter=128125\n",
      "#42 SAVE_CHECKPOINT: step=42000000 counter=131250\n",
      "#43 SAVE_CHECKPOINT: step=43000000 counter=134375\n",
      "#44 SAVE_CHECKPOINT: step=44000000 counter=137500\n",
      "#45 SAVE_CHECKPOINT: step=45000000 counter=140625\n",
      "#46 SAVE_CHECKPOINT: step=46000000 counter=143750\n",
      "#47 SAVE_CHECKPOINT: step=47000000 counter=146875\n",
      "#48 SAVE_CHECKPOINT: step=48000000 counter=150000\n",
      "#49 SAVE_CHECKPOINT: step=49000000 counter=153125\n",
      "#50 SAVE_CHECKPOINT: step=50000000 counter=156250\n",
      "#51 SAVE_CHECKPOINT: step=51000000 counter=159375\n",
      "#52 SAVE_CHECKPOINT: step=52000000 counter=162500\n",
      "#53 SAVE_CHECKPOINT: step=53000000 counter=165625\n",
      "#54 SAVE_CHECKPOINT: step=54000000 counter=168750\n",
      "#55 SAVE_CHECKPOINT: step=55000000 counter=171875\n",
      "#56 SAVE_CHECKPOINT: step=56000000 counter=175000\n",
      "#57 SAVE_CHECKPOINT: step=57000000 counter=178125\n",
      "#58 SAVE_CHECKPOINT: step=58000000 counter=181250\n",
      "#59 SAVE_CHECKPOINT: step=59000000 counter=184375\n",
      "#60 SAVE_CHECKPOINT: step=60000000 counter=187500\n",
      "#61 SAVE_CHECKPOINT: step=61000000 counter=190625\n",
      "#62 SAVE_CHECKPOINT: step=62000000 counter=193750\n",
      "#63 SAVE_CHECKPOINT: step=63000000 counter=196875\n",
      "#64 SAVE_CHECKPOINT: step=64000000 counter=200000\n",
      "#65 SAVE_CHECKPOINT: step=65000000 counter=203125\n",
      "#66 SAVE_CHECKPOINT: step=66000000 counter=206250\n",
      "#67 SAVE_CHECKPOINT: step=67000000 counter=209375\n",
      "#68 SAVE_CHECKPOINT: step=68000000 counter=212500\n",
      "#69 SAVE_CHECKPOINT: step=69000000 counter=215625\n",
      "#70 SAVE_CHECKPOINT: step=70000000 counter=218750\n",
      "#71 SAVE_CHECKPOINT: step=71000000 counter=221875\n",
      "#72 SAVE_CHECKPOINT: step=72000000 counter=225000\n",
      "#73 SAVE_CHECKPOINT: step=73000000 counter=228125\n",
      "#74 SAVE_CHECKPOINT: step=74000000 counter=231250\n",
      "#75 SAVE_CHECKPOINT: step=75000000 counter=234375\n",
      "#76 SAVE_CHECKPOINT: step=76000000 counter=237500\n",
      "#77 SAVE_CHECKPOINT: step=77000000 counter=240625\n",
      "#78 SAVE_CHECKPOINT: step=78000000 counter=243750\n",
      "#79 SAVE_CHECKPOINT: step=79000000 counter=246875\n",
      "#80 SAVE_CHECKPOINT: step=80000000 counter=250000\n",
      "seconds: 0.10978388786315918\n",
      "time: 0:00:00.109784\n"
     ]
    }
   ],
   "source": [
    "def test_stat_saving(max_global_steps, checkpoint_interval, eval_every,\n",
    "               num_emulators=32, max_local_steps=10, verbose=2, dump_fun=None):\n",
    "    global_step = 0\n",
    "    counter = 0\n",
    "    last_saving_step = 0\n",
    "    \n",
    "    data = dict(num_eval=0, num_save=0)\n",
    "    def eval_func(verbose=False):\n",
    "        data['num_eval'] += 1\n",
    "        if verbose > 1:\n",
    "            print('#{0} EVALUATE: step={1} counter={2}'.format(data['num_eval'], global_step, counter))\n",
    "        return TrainingStats(*[global_step + i for i in range(9)])\n",
    "        \n",
    "    def save_func(l, verbose=False):\n",
    "        data['num_save'] += 1\n",
    "        if verbose > 0:\n",
    "            print('#{0} SAVE_CHECKPOINT: step={1} counter={2}'.format(data['num_save'], global_step, counter))\n",
    "        if (dump_fun is not None) and len(l) > 0:\n",
    "            dump_fun(l)\n",
    "    l = []\n",
    "    while global_step < max_global_steps:\n",
    "        global_step += num_emulators * max_local_steps\n",
    "        counter += 1\n",
    "        \n",
    "        if counter % (eval_every // (num_emulators * max_local_steps)) == 0:\n",
    "            stats = eval_func(verbose)\n",
    "            l.append(stats)\n",
    "            \n",
    "        if global_step - last_saving_step >= checkpoint_interval:\n",
    "            last_saving_step = global_step\n",
    "            save_func(l, verbose)\n",
    "            l = []\n",
    "            \n",
    "    save_func(l)\n",
    "    \n",
    "import time, datetime\n",
    "with open('test/test_pickle.p', 'ba') as file:            \n",
    "    pickler = pkl.Pickler(file, protocol=4)\n",
    "    #dump = None\n",
    "    dump = pickler.dump\n",
    "    #dump = lambda o: pkl.dump(o, file, protocol=4)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_stat_saving(max_global_steps=80*(10**6), checkpoint_interval=10**6,\n",
    "                       eval_every=10240, verbose=1, dump_fun=dump)\n",
    "    delta_t = time.time() - start_time\n",
    "    print('seconds:', delta_t)\n",
    "    print('time:', datetime.timedelta(seconds=delta_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "def load_pickled(filename):\n",
    "    with open(filename, 'br') as file:   \n",
    "        unpickler = pkl.Unpickler(file)\n",
    "        data = []\n",
    "        while True:\n",
    "            try:\n",
    "                data.extend(unpickler.load())\n",
    "            except EOFError as e:\n",
    "                break\n",
    "    return data\n",
    "        \n",
    "data = load_pickled('test/test_pickle.p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_size = len(data)*9*getsizeof(2.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.609222412109375"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_size / (1024*1024) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
